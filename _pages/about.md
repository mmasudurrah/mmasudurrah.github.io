---
layout: about
title: About
permalink: /
subtitle: 

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>PostDoc @PurdueEngineers (IE), Ph.D in CS@Purdue</p>
    <p>Email: rahman64@purdue.edu</p>
    <div class="social"> 
    <div class="contact-icons">
    <a href="https://scholar.google.com/citations?user=0nUv7b0AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>&nbsp;<a href="https://mmasudurrah.github.io/assets/pdf/CV_Md_Masudur_Rahman.pdf" title="CV" rel="external nofollow noopener" target="_blank"><i class="ai ai-cv"></i></a>
    </div>
    </div>
news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---
My research focuses on developing **principled and adaptable intelligence** for autonomous systems operating in **high-stakes environments** where errors carry significant cost and adaptability is essential. I aim to bridge the gap between powerful simulation-trained agents and the unpredictable demands of the real world by unifying **reinforcement learning (RL)** with **vision-language models (VLM)** to enable systems that **reason, adapt, and act** under uncertainty. This work advances both **generalization and sample efficiency** in decision-making and integrates learning with structured reasoning to produce agents capable of grounded, interactive behaviors. These contributions drive **real-world impact** in domains such as **burn diagnosis through medical imaging** and **emergency robotics**, where systems must perceive affordances and improvise actions in unstructured, rapidly evolving conditions.

I am currently a Postdoctoral Research Assistant in the Edwardson School of Industrial Engineering at Purdue University, working with **Dr. Juan P. Wachs**. I completed my Ph.D. in Computer Science at Purdue University in 2024 under the supervision of Dr.  [Yexiang Xue](https://www.cs.purdue.edu/homes/yexiang/). I completed my M.S. in Computer Science at the University of Virginia in 2018. Before that, I worked as a Lecturer at BRAC University from 2013 to 2015, after earning my B.Sc. in Computer Science and Engineering from BUET in 2013.

### RESEARCH HIGHLIGHTS
---
My research focuses on developing intelligent systems that are both **principled**, meaning they are grounded in rigorous, interpretable methods, and **adaptable**, meaning they can generalize, reason, and improvise in dynamic, **high-stakes environments**. These environments are characterized by uncertainty, limited data, time-critical decision-making, and serious consequences for failure. Real-world situations such as treating trauma patients or deploying robots in disaster zones vividly illustrate these challenges, where objectives shift rapidly, information is incomplete, and decisions must be made under pressure. Ironically, **these are the very contexts where we most need AI**, yet current systems often fall short in delivering robust and reliable performance. Overcoming these limitations requires a new generation of intelligent systems that combine real-time decision-making with contextual understanding and principled reasoning.

To meet these challenges, I design methods that combine **reinforcement learning** with **foundation models**, such as **vision-language models**, to enable agents that learn efficiently, act robustly, and generalize meaningfully beyond their training distributions. My work advances algorithms that smooth the optimization landscape to improve stability and generalization, while incorporating symbolic abstractions, procedural planning, and logic-based verification to support decision-making in ambiguous, under-specified, or novel scenarios.  

In high-stakes clinical settings (**AI in Healthcare**), my **burn diagnosis** system achieved 95% accuracy in surgical decision-making using **real-patient data** from a regional burn center, significantly outperforming traditional methods and clinician judgment, which typically achieve 70% accuracy. For **remote robotic surgery**, my teleoperation framework maintained effective control under network **delays of up to five seconds**, whereas conventional systems often fail beyond 300 milliseconds in **emulated settings**. These capabilities demonstrate strong potential for greater autonomy in low-bandwidth and resource-constrained environments.

My research is organized into three interconnected thrusts:  (1) learning **generalizable and sample-efficient** policies for decision-making;  (2) enabling agents to **reason and improvise** in uncertain and data-limited settings; and  (3) deploying these capabilities in **embodied systems** and safety-critical domains such as autonomous surgery, trauma response, and remote diagnostics. Across these directions, I aim to build AI systems that are both technically robust and structurally reliable, capable of performing with resilience, transparency, and trust in the unpredictable complexity of the real world.


