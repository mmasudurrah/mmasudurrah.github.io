---
layout: about
title: About
permalink: /
subtitle: 

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>PostDoc @PurdueEngineers (IE), Ph.D in CS@Purdue</p>
    <p>Email: rahman64@purdue.edu</p>
    <div class="social"> 
    <div class="contact-icons">
    <a href="https://scholar.google.com/citations?user=0nUv7b0AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>&nbsp;<a href="https://mmasudurrah.github.io/assets/pdf/CV_Md_Masudur_Rahman.pdf" title="CV" rel="external nofollow noopener" target="_blank"><i class="ai ai-cv"></i></a>
    </div>
    </div>
news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---
My research focuses on developing **principled and adaptable intelligence** for autonomous systems operating in **high-stakes environments** where errors carry significant cost and adaptability is essential. I aim to bridge the gap between powerful simulation-trained agents and the unpredictable demands of the real world by unifying **reinforcement learning (RL)** with **vision-language models (VLM)** to enable systems that **reason, adapt, and act** under uncertainty. This work advances both **generalization and sample efficiency** in decision-making and integrates learning with structured reasoning to produce agents capable of grounded, interactive behaviors. These contributions drive **real-world impact** in domains such as **burn diagnosis through medical imaging** and **emergency robotics**, where systems must perceive affordances and improvise actions in unstructured, rapidly evolving conditions.

I am currently a Postdoctoral Research Assistant in the Edwardson School of Industrial Engineering at Purdue University, working with **Dr. Juan P. Wachs**. I completed my Ph.D. in Computer Science at Purdue University in 2024 under the supervision of Dr.  [Yexiang Xue](https://www.cs.purdue.edu/homes/yexiang/). I completed my M.S. in Computer Science at the University of Virginia in 2018. Before that, I worked as a Lecturer at BRAC University from 2013 to 2015, after earning my B.Sc. in Computer Science and Engineering from BUET in 2013.

### KEY RESEARCH AREAS
---
### Learning, Reasoning & Decision-Making
Modern decision-making agents are remarkably powerful in simulation, yet struggle to adapt, generalize, or explain their behavior in complex real-world tasks. Deep reinforcement learning (**RL**) methods often overfit narrow tasks and lack robustness under distributional shift, while multimodal foundation models (e.g., **VLM**) offer rich priors but operate largely without causal structure or grounded planning. This disconnect limits the deployment of intelligent systems in dynamic environments where adaptability and transparency are essential. My research addresses this gap by integrating sample-efficient reinforcement learning with structured reasoning, combining symbolic logic, procedural planning, and vision-language representations to build agents that **generalize, improvise**, and justify their actions beyond static benchmarks.


### Embodied Autonomy  
Despite advances in robotic control and learning, true autonomy in **unstructured and high-stakes environments** remains elusive. Robots deployed in the real world must contend with noisy sensors, changing dynamics, partial observability, and limited human oversight, all under strict constraints of time, bandwidth, and safety. These challenges are amplified in critical domains such as remote surgery or field robotics, where teleoperation is unreliable and scripted behavior fails. My research develops embodied systems that integrate **predictive shared autonomy, affordance-aware planning, and real-time improvisation** to enable robust task execution in settings marked by uncertainty, delay, and sparse supervision.


### AI in Healthcare
While AI has demonstrated early success in medical imaging and language-based triage, current models often lack the transparency, adaptability, and real-world integration needed for deployment in high-stakes clinical care (e.g., **Burn Diagnosis**). Most models rely on large, clean datasets and provide limited explanation, which is a mismatch for domains such as trauma response, rural surgery, or battlefield medicine, where data is noisy and mistakes carry real cost. My research bridges this gap by building multimodal (e.g., **medical imaging** - ultrasound, Doppler), interpretable decision-support systems grounded in **clinical reasoning**, procedural knowledge, and symbolic logic. These systems operate in **austere settings** and outperform traditional models in surgical intervention planning, diagnosis, and adaptive triage.

